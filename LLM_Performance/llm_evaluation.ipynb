{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter # Import Counter for debugging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Baseline Model Performance ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      1.00      0.78         9\n",
      "    positive       1.00      0.76      0.86        21\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.82      0.88      0.82        30\n",
      "weighted avg       0.89      0.83      0.84        30\n",
      "\n",
      "Starting Groq LLM evaluation...\n",
      "Processed 10/30 reviews...\n",
      "Processed 20/30 reviews...\n",
      "Processed 30/30 reviews...\n",
      "Groq LLM evaluation complete!\n",
      "\n",
      "=== DEBUGGING OUTPUT ===\n",
      "First 10 True Labels: ['positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative']\n",
      "First 10 LLM Predictions: ['positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative']\n",
      "\n",
      "Unique True Labels: {'positive', 'negative'}\n",
      "Unique LLM Predictions: {'positive', 'negative'}\n",
      "\n",
      "Prediction Counts: Counter({'positive': 18, 'negative': 12})\n",
      "=======================\n",
      "\n",
      "### LLM Performance (using Groq & Llama) ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      1.00      0.86         9\n",
      "    positive       1.00      0.86      0.92        21\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.88      0.93      0.89        30\n",
      "weighted avg       0.93      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Load Data\n",
    "df = pd.read_csv('IMDb Dataset.csv')\n",
    "# Create a small test set (e.g., 100 samples)\n",
    "train_df, test_df = train_test_split(df, test_size=100, random_state=42)\n",
    "# Save the test set reviews and labels\n",
    "test_reviews = test_df['review'].tolist()\n",
    "true_labels = test_df['sentiment'].tolist()\n",
    "\n",
    "# Phase 2: Baseline Model (on the larger training set)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_df['review'])\n",
    "y_train = train_df['sentiment']\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vectorize the test set for the baseline model\n",
    "X_test = vectorizer.transform(test_reviews)\n",
    "baseline_predictions = model.predict(X_test)\n",
    "\n",
    "print(\"### Baseline Model Performance ###\")\n",
    "print(classification_report(true_labels, baseline_predictions))\n",
    "\n",
    "# Phase 3: LLM Evaluation\n",
    "\n",
    "# Initialize the Groq \n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "llm_predictions = []\n",
    "\n",
    "print(\"Starting Groq LLM evaluation...\")\n",
    "for i, review in enumerate(test_reviews):\n",
    "    # Craft your prompt\n",
    "    prompt = f\"\"\"\n",
    "    Classify the following movie review sentiment as only 'positive' or 'negative'. Do not write anything else.\n",
    "\n",
    "    Review: \\\"{review}\\\"\n",
    "    Sentiment:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use the Groq client\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        # Get the raw response and clean it robustly\n",
    "        raw_response = response.choices[0].message.content.strip().lower()\n",
    "        \n",
    "        # Extract the sentiment from the raw response\n",
    "        if 'positive' in raw_response:\n",
    "            prediction = 'positive'\n",
    "        elif 'negative' in raw_response:\n",
    "            prediction = 'negative'\n",
    "        else:\n",
    "            # If it doesn't find either, mark it as an error and see what the weird response was.\n",
    "            print(f\"Unexpected response for review {i}: '{raw_response}'. Marking as error.\")\n",
    "            prediction = 'error'\n",
    "            \n",
    "        llm_predictions.append(prediction)\n",
    "        \n",
    "        # Print a progress update every 10 reviews\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(test_reviews)} reviews...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with review {i}: {e}\")\n",
    "        llm_predictions.append(\"error\")\n",
    "    \n",
    "    time.sleep(0.5)  # Short pause\n",
    "\n",
    "print(\"Groq LLM evaluation complete!\\n\")\n",
    "\n",
    "# --- DEBUGGING: Check what we actually got ---\n",
    "print(\"=== DEBUGGING OUTPUT ===\")\n",
    "print(\"First 10 True Labels:\", true_labels[:10])\n",
    "print(\"First 10 LLM Predictions:\", llm_predictions[:10])\n",
    "\n",
    "print(\"\\nUnique True Labels:\", set(true_labels))\n",
    "print(\"Unique LLM Predictions:\", set(llm_predictions))\n",
    "\n",
    "prediction_counts = Counter(llm_predictions)\n",
    "print(\"\\nPrediction Counts:\", prediction_counts)\n",
    "print(\"=======================\\n\")\n",
    "\n",
    "# --- Now calculate the final performance ---\n",
    "print(\"### LLM Performance (using Groq & Llama) ###\")\n",
    "print(classification_report(true_labels, llm_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ERROR ANALYSIS\n",
      "==================================================\n",
      "Number of errors: 3 out of 30 reviews (10.00%)\n",
      "\n",
      "Let's look at some of the errors:\n",
      "\n",
      "True: positive | Predicted: negative\n",
      "Review: This movie starts off somewhat slowly and gets running towards the end. Not that that is bad, it was done to illustrate character trait degression of the main character. Consequently, if you are not i...\n",
      "--------------------------------------------------------------------------------\n",
      "True: positive | Predicted: negative\n",
      "Review: The production quality, cast, premise, authentic New England (Waterbury, CT?) locale and lush John Williams score should have resulted in a 3-4 star collectors item. Unfortunately, all we got was a pa...\n",
      "--------------------------------------------------------------------------------\n",
      "True: positive | Predicted: negative\n",
      "Review: I've never really been sure whether I liked this documentary or not. It was shown on Channel 4 before a cut down version of Revelations, and is on the Revelations video tape before the uncut show. The...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error Type Breakdown:\n",
      "False Positives (model said POSITIVE but was NEGATIVE): 0\n",
      "False Negatives (model said NEGATIVE but was POSITIVE): 3\n"
     ]
    }
   ],
   "source": [
    "# === ERROR ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "#  Create a DataFrame for easy analysis\n",
    "\n",
    "error_analysis_df = pd.DataFrame({\n",
    "    'True_Label': true_labels,\n",
    "    'LLM_Prediction': llm_predictions,\n",
    "    'Review': test_reviews\n",
    "})\n",
    "\n",
    "# Adding a column to flag incorrect predictions\n",
    "error_analysis_df['Correct'] = error_analysis_df['True_Label'] == error_analysis_df['LLM_Prediction']\n",
    "\n",
    "#  Filter to only show the mistakes\n",
    "mistakes_df = error_analysis_df[error_analysis_df['Correct'] == False]\n",
    "\n",
    "# Print a summary\n",
    "num_errors = len(mistakes_df)\n",
    "total_reviews = len(error_analysis_df)\n",
    "print(f\"Number of errors: {num_errors} out of {total_reviews} reviews ({num_errors/total_reviews:.2%})\")\n",
    "\n",
    "if num_errors > 0:\n",
    "    print(\"\\nLet's look at some of the errors:\\n\")\n",
    "    # Display the first 5-10 errors for inspection\n",
    "    for i, row in mistakes_df.head(10).iterrows():\n",
    "        print(f\"True: {row['True_Label']} | Predicted: {row['LLM_Prediction']}\")\n",
    "        print(f\"Review: {row['Review'][:200]}...\") # Show first 200 chars to avoid huge output\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"Perfect score! No errors to analyze.\")\n",
    "\n",
    "# Count error types\n",
    "if num_errors > 0:\n",
    "    print(\"\\nError Type Breakdown:\")\n",
    "    # Count how many times the model predicted 'positive' when it was actually 'negative' (False Positive)\n",
    "    false_positives = len(mistakes_df[(mistakes_df['True_Label'] == 'negative') & (mistakes_df['LLM_Prediction'] == 'positive')])\n",
    "    # Count how many times the model predicted 'negative' when it was actually 'positive' (False Negative)\n",
    "    false_negatives = len(mistakes_df[(mistakes_df['True_Label'] == 'positive') & (mistakes_df['LLM_Prediction'] == 'negative')])\n",
    "    \n",
    "    print(f\"False Positives (model said POSITIVE but was NEGATIVE): {false_positives}\")\n",
    "    print(f\"False Negatives (model said NEGATIVE but was POSITIVE): {false_negatives}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
